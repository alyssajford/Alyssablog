---
layout: post
title:  "What Should I Have For Dessert?"
date: 2024-10-23
author: Alyssa Ford
description: If you love sugar as much as me, you treasure the moment in the day where you can have a sweet treat. This post will help you to develop webscrapping skills and dive deeper into the sugar world.
image: "/assets/images/cookie_cover.jpg"

---

<h1> <em><strong> DESSERT </strong></em></h1> The Best Meal Of the Day

Introduction:

Growing up, my family loved treats. Every weekend we would have some sort of cookie or baked good in our pantry. I've had a sweet tooth my whole life and I truly believe sugar makes everyone happy.
Simply put, life is better with sugar. Now, the real question is: <em><strong> what is the best dessert out there? </strong></em></h1>

That is why I consulted a well-known site that ranks various things. That website is Food.com. They recently compiled a top 100 list for dessert recipes cookies. Perfect!
After browsing through these recipes (200 combined with dessert and cookies), I wanted to examine what made a recipe special enough to be in the top 100 list. This would require
making a dataset of my own.

If you'd like to view these websites before the post here they are!
[Dessert](https://www.food.com/ideas/top-dessert-recipes-6930?ref=nav#c-791391)
[Cookies](https://www.food.com/ideas/cookie-recipes-7152?ref=nav#c-932718)

Luckily, BeautifulSoup allows us to create our own datasets from websites. Food.com allows for webscrapping 
and with these two tools, we are good to go. The first step in creating a dataset from a website is to import necessary libraries from Python. Here are the libraries I used to get my dataset, and I would recommend starting wiht these as you create your own dataset! 

Next, we import the URL and check it using these functions

As we continue in the webscraping process, it is crucial to make sure the website you use allows for webscrapping. Check the robot.txt file to verify the rules the website has. In this example, the website allows for webscrapping as per the robot.txt file. 

Gathering data from a website can be tricky, but we will walk through the specific steps


